"""OpenAI é›†æˆæ¨¡å—

æä¾› AI å¢å¼ºåŠŸèƒ½ï¼Œä»…ä¾èµ– OpenAI APIã€‚
"""

import os
from dataclasses import dataclass
from typing import Optional, List
from openai import OpenAI

from .prompts import (
    PROMPT_ENHANCE,
    PROMPT_SUGGEST,
    CHAT_SYSTEM_PROMPT,
)


@dataclass
class AIConfig:
    """AI é…ç½®"""
    api_key: str
    model: str = "gpt-4o-mini"
    max_tokens: int = 300
    temperature: float = 0.7


class AIHandler:
    """OpenAI å¤„ç†å™¨"""

    def __init__(self, config: Optional[AIConfig] = None):
        if config is None:
            config = AIConfig(
                api_key=os.getenv("OPENAI_API_KEY", ""),
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            )
        if not config.api_key:
            raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

        self.config = config
        # æ”¯æŒ OPENAI_BASE_URL ç¯å¢ƒå˜é‡ï¼ˆå¦‚æ™ºè°± AIï¼‰
        base_url = os.getenv("OPENAI_BASE_URL")
        self.client = OpenAI(api_key=config.api_key, base_url=base_url)

    def _should_disable_thinking(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼ˆGLM-4.x ç³»åˆ—ï¼‰"""
        return self.config.model.startswith("glm-4")

    def enhance_input(self, text: str) -> str:
        """AI ä¼˜åŒ–ä»»åŠ¡æè¿°

        Args:
            text: åŸå§‹ä»»åŠ¡æ–‡æœ¬

        Returns:
            ä¼˜åŒ–åçš„ä»»åŠ¡æè¿°ï¼ˆå¦‚æœ AI è¿”å›ç©ºåˆ™è¿”å›åŸå§‹æ–‡æœ¬ï¼‰
        """
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": PROMPT_ENHANCE.format(text=text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        enhanced = response.choices[0].message.content.strip()
        # å›é€€æœºåˆ¶ï¼šå¦‚æœ AI è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
        return enhanced if enhanced else text

    def suggest_next(self, todos: List) -> str:
        """AI å»ºè®®ä¸‹ä¸€æ­¥

        Args:
            todos: ä»»åŠ¡åˆ—è¡¨

        Returns:
            å»ºè®®æ–‡æœ¬
        """
        # è¿‡æ»¤æœªå®Œæˆçš„ä»»åŠ¡
        incomplete_todos = [t for t in todos if not t.done]

        if not incomplete_todos:
            return "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼"

        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority}, {'å·²å®Œæˆ' if t.done else 'æœªå®Œæˆ'})"
            for t in incomplete_todos
        ])

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": PROMPT_SUGGEST.format(todos=todos_text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": 0.7,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        return response.choices[0].message.content.strip()

    def chat(self, user_input: str, todos: List) -> str:
        """AI å¯¹è¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            todos: ä»»åŠ¡åˆ—è¡¨

        Returns:
            AI å›å¤
        """
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority})"
            for t in todos
        ])

        # ä½¿ç”¨ prompts.py ä¸­çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = CHAT_SYSTEM_PROMPT.format(todos=todos_text)

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            "max_tokens": 300,
            "temperature": 0.8,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        return response.choices[0].message.content.strip()

    def suggest_next_stream(self, todos: List):
        """AI å»ºè®®ä¸‹ä¸€æ­¥ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            todos: ä»»åŠ¡åˆ—è¡¨

        Yields:
            å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        # è¿‡æ»¤æœªå®Œæˆçš„ä»»åŠ¡
        incomplete_todos = [t for t in todos if not t.done]

        if not incomplete_todos:
            yield "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼"
            return

        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority}, {'å·²å®Œæˆ' if t.done else 'æœªå®Œæˆ'})"
            for t in incomplete_todos
        ])

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": PROMPT_SUGGEST.format(todos=todos_text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": 0.7,
            "stream": True,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        for chunk in self.client.chat.completions.create(**params):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def chat_stream(self, user_input: str, todos: List):
        """AI å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            todos: ä»»åŠ¡åˆ—è¡¨

        Yields:
            å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority})"
            for t in todos
        ])

        # ä½¿ç”¨ prompts.py ä¸­çš„ç³»ç»Ÿæç¤ºè¯
        system_prompt = CHAT_SYSTEM_PROMPT.format(todos=todos_text)

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            "max_tokens": 300,
            "temperature": 0.8,
            "stream": True,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        for chunk in self.client.chat.completions.create(**params):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


def get_ai_handler() -> AIHandler:
    """è·å– AI å¤„ç†å™¨å®ä¾‹

    ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®å¹¶åˆ›å»ºå¤„ç†å™¨

    Returns:
        AIHandler å®ä¾‹

    Raises:
        ValueError: å¦‚æœç¼ºå°‘ OPENAI_API_KEY
    """
    api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

    return AIHandler(AIConfig(api_key=api_key, model=model))
