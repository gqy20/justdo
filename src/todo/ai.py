"""OpenAI é›†æˆæ¨¡å—

æä¾› AI å¢å¼ºåŠŸèƒ½ï¼Œä»…ä¾èµ– OpenAI APIã€‚
"""

import os
from dataclasses import dataclass
from typing import Optional, List
from openai import OpenAI


@dataclass
class AIConfig:
    """AI é…ç½®"""
    api_key: str
    model: str = "gpt-4o-mini"
    max_tokens: int = 300
    temperature: float = 0.7


class AIHandler:
    """OpenAI å¤„ç†å™¨"""

    # æç¤ºè¯æ¨¡æ¿
    PROMPT_ENHANCE = """ä½ æ˜¯ä»»åŠ¡æè¿°ä¼˜åŒ–ä¸“å®¶ã€‚å°†æ¨¡ç³Šçš„ä»»åŠ¡æè¿°è½¬åŒ–ä¸ºå…·ä½“ã€å¯æ‰§è¡Œçš„è¡ŒåŠ¨ã€‚

éœ€è¦ä¼˜åŒ–çš„æƒ…å†µï¼š
- å¤ªæ¨¡ç³Šï¼šçœ‹ä¹¦ã€å­¦ä¹ ã€è¿åŠ¨ â†’ é˜…è¯»ç¬¬1ç« ã€å­¦ä¹ PythonåŸºç¡€ã€æ™¨è·‘3å…¬é‡Œ
- ç¼ºå°‘åŠ¨è¯ï¼šæŠ¥å‘Šã€ä¼šè®® â†’ æ’°å†™æŠ¥å‘Šã€å‚åŠ è¯„å®¡ä¼šè®®
- æ²¡æœ‰å…·ä½“å†…å®¹ï¼šä»£ç ã€æ–‡æ¡£ â†’ ä¿®å¤ç™»å½•bugã€æ›´æ–°APIæ–‡æ¡£

ä¼˜åŒ–åŸåˆ™ï¼š
1. æ·»åŠ å…·ä½“çš„è¡ŒåŠ¨åŠ¨è¯ï¼ˆæ’°å†™ã€é˜…è¯»ã€å®Œæˆã€ä¿®å¤ï¼‰
2. æ˜ç¡®å…·ä½“çš„å†…å®¹æˆ–æ•°é‡
3. ä¿æŒç®€æ´ï¼ˆ5-12å­—ï¼‰
4. æ€»æ˜¯å°è¯•æ”¹è¿›ï¼Œé™¤éåŸæ–‡å·²ç»å¾ˆå®Œç¾

åŸæ–‡ï¼š{text}

ä¼˜åŒ–åçš„æè¿°ï¼ˆç›´æ¥è¾“å‡ºï¼Œä¸è¦è§£é‡Šï¼‰ï¼š"""
    PROMPT_SUGGEST = """æ ¹æ®å¾…åŠä»»åŠ¡åˆ—è¡¨ï¼Œåˆ†æå¹¶å»ºè®®ä¸‹ä¸€æ­¥åšå“ªä¸ªä»»åŠ¡ã€‚

ä»»åŠ¡åˆ—è¡¨ï¼š
{todos}

è¦æ±‚ï¼š
1. åªå»ºè®®ä¸€ä¸ªä»»åŠ¡
2. åˆ†æç†ç”±ï¼ˆ100-200å­—ï¼‰
3. ä»ä¼˜å…ˆçº§ã€ç´§æ€¥ç¨‹åº¦ã€å¿ƒç†é˜»åŠ›ä¸‰ä¸ªç»´åº¦åˆ†æ
4. è¾“å‡ºæ ¼å¼ï¼šğŸ’¡ å»ºè®®ä¼˜å…ˆå®Œæˆ [ä»»åŠ¡ID]

ç›´æ¥è¾“å‡ºå»ºè®®ï¼š"""

    def __init__(self, config: Optional[AIConfig] = None):
        if config is None:
            config = AIConfig(
                api_key=os.getenv("OPENAI_API_KEY", ""),
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            )
        if not config.api_key:
            raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

        self.config = config
        # æ”¯æŒ OPENAI_BASE_URL ç¯å¢ƒå˜é‡ï¼ˆå¦‚æ™ºè°± AIï¼‰
        base_url = os.getenv("OPENAI_BASE_URL")
        self.client = OpenAI(api_key=config.api_key, base_url=base_url)

    def _should_disable_thinking(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼ˆGLM-4.x ç³»åˆ—ï¼‰"""
        return self.config.model.startswith("glm-4")

    def enhance_input(self, text: str) -> str:
        """AI ä¼˜åŒ–ä»»åŠ¡æè¿°

        Args:
            text: åŸå§‹ä»»åŠ¡æ–‡æœ¬

        Returns:
            ä¼˜åŒ–åçš„ä»»åŠ¡æè¿°ï¼ˆå¦‚æœ AI è¿”å›ç©ºåˆ™è¿”å›åŸå§‹æ–‡æœ¬ï¼‰
        """
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": self.PROMPT_ENHANCE.format(text=text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": self.config.temperature,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        enhanced = response.choices[0].message.content.strip()
        # å›é€€æœºåˆ¶ï¼šå¦‚æœ AI è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬
        return enhanced if enhanced else text

    def suggest_next(self, todos: List) -> str:
        """AI å»ºè®®ä¸‹ä¸€æ­¥

        Args:
            todos: ä»»åŠ¡åˆ—è¡¨

        Returns:
            å»ºè®®æ–‡æœ¬
        """
        # è¿‡æ»¤æœªå®Œæˆçš„ä»»åŠ¡
        incomplete_todos = [t for t in todos if not t.done]

        if not incomplete_todos:
            return "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼"

        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority}, {'å·²å®Œæˆ' if t.done else 'æœªå®Œæˆ'})"
            for t in incomplete_todos
        ])

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": self.PROMPT_SUGGEST.format(todos=todos_text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": 0.7,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        return response.choices[0].message.content.strip()

    def chat(self, user_input: str, todos: List) -> str:
        """AI å¯¹è¯

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            todos: ä»»åŠ¡åˆ—è¡¨

        Returns:
            AI å›å¤
        """
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority})"
            for t in todos
        ])

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ Todo åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·ç®¡ç†ä»»åŠ¡å’Œå…‹æœæ‹–å»¶ã€‚

å½“å‰ä»»åŠ¡åˆ—è¡¨ï¼š
{todos_text}

å›ç­”è¦ç®€æ´ã€æœ‰åŒç†å¿ƒã€å®ç”¨ã€‚"""

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            "max_tokens": 300,
            "temperature": 0.8,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        response = self.client.chat.completions.create(**params)
        return response.choices[0].message.content.strip()

    def suggest_next_stream(self, todos: List):
        """AI å»ºè®®ä¸‹ä¸€æ­¥ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            todos: ä»»åŠ¡åˆ—è¡¨

        Yields:
            å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        # è¿‡æ»¤æœªå®Œæˆçš„ä»»åŠ¡
        incomplete_todos = [t for t in todos if not t.done]

        if not incomplete_todos:
            yield "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼"
            return

        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority}, {'å·²å®Œæˆ' if t.done else 'æœªå®Œæˆ'})"
            for t in incomplete_todos
        ])

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "user", "content": self.PROMPT_SUGGEST.format(todos=todos_text)}
            ],
            "max_tokens": self.config.max_tokens,
            "temperature": 0.7,
            "stream": True,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        for chunk in self.client.chat.completions.create(**params):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    def chat_stream(self, user_input: str, todos: List):
        """AI å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            todos: ä»»åŠ¡åˆ—è¡¨

        Yields:
            å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        todos_text = "\n".join([
            f"- [{t.id}] {t.text} (ä¼˜å…ˆçº§: {t.priority})"
            for t in todos
        ])

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„ Todo åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·ç®¡ç†ä»»åŠ¡å’Œå…‹æœæ‹–å»¶ã€‚

å½“å‰ä»»åŠ¡åˆ—è¡¨ï¼š
{todos_text}

å›ç­”è¦ç®€æ´ã€æœ‰åŒç†å¿ƒã€å®ç”¨ã€‚"""

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.config.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ],
            "max_tokens": 300,
            "temperature": 0.8,
            "stream": True,
        }
        # GLM-4.x éœ€è¦ç¦ç”¨æ€è€ƒæ¨¡å¼ä»¥åŠ å¿«é€Ÿåº¦
        if self._should_disable_thinking():
            params["extra_body"] = {"thinking": {"type": "disabled"}}

        for chunk in self.client.chat.completions.create(**params):
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content


def get_ai_handler() -> AIHandler:
    """è·å– AI å¤„ç†å™¨å®ä¾‹

    ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®å¹¶åˆ›å»ºå¤„ç†å™¨

    Returns:
        AIHandler å®ä¾‹

    Raises:
        ValueError: å¦‚æœç¼ºå°‘ OPENAI_API_KEY
    """
    api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        raise ValueError("OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")

    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

    return AIHandler(AIConfig(api_key=api_key, model=model))
